<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="HPSv3: Towards Wide-Spectrum Human Preference Score (ICCV 2025)">
  <meta property="og:title" content="HPSv3: Towards Wide-Spectrum Human Preference Score"/>
  <meta property="og:description" content="HPSv3: A state-of-the-art human preference score model for evaluating image quality and prompt alignment"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="HPSv3: Towards Wide-Spectrum Human Preference Score">
  <meta name="twitter:description" content="HPSv3: A state-of-the-art human preference score model for evaluating image quality and prompt alignment">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="HPSv3, Human Preference Score, Image Quality Assessment, ICCV 2025, Computer Vision, Image Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>HPSv3: Towards Wide-Spectrum Human Preference Score</title>
  <link rel="icon" type="image/x-icon" href="static/images/human_preference_score_v3_icon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <style>
    body {
      zoom: 1.25;
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">HPSv3: Towards Wide-Spectrum Human Preference Score</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yuhang-ma.github.io" target="_blank">Yuhang Ma</a><sup>1,2*</sup>,</span>
                <span class="author-block">
                  <a href="https://xilanhua12138.github.io" target="_blank">Yunhao Shui</a><sup>1,3*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=cnOAMbUAAAAJ" target="_blank">Xiaoshi Wu</a><sup>4</sup>,</span>
                  <span class="author-block">
                    <a href="https://keqiangsun.github.io" target="_blank">Keqiang Sun</a><sup>1,4†</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=zh-CN" target="_blank">Hongsheng Li</a><sup>4,5,6†</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Mizzen AI&ensp;&ensp; <sup>2</sup>King's College London&ensp;&ensp; <sup>3</sup>Shanghai Jiaotong University&ensp;&ensp; <sup>4</sup>CUHK MMLab&ensp;&ensp; <br><sup>5</sup>Shanghai AI Laboratory&ensp;&ensp; <sup>6</sup>CPII, InnoHK<br>ICCV 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution&ensp;&ensp; <sup>†</sup>Equal Advising</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2411.07232.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/MizzenAI/HPSv3" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.07232" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="has-text-centered"></div>
        <img src="static/images/teaser.png" alt="Teaser Image" style="max-width:110%; height:auto; display:inline-block; transform:translateX(-5%);">
      <h2 class="subtitle has-text-centered">
        Examples from the HPDv3 dataset with their corresponding HPSv3 scores. HPDv3 represents the first comprehensive wide-spectrum human preference dataset, designed to evaluate generative models across a wide range of prompts and scenarios.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h1 class="title is-3">Abstract</h1>
        <div class="content has-text-justified">
          <p>
          Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking.
          Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step.
          % Our contributions provide a robust benchmark for full-spectrum evaluation and introduce a scalable, human-aligned approach to improving image generation quality.
          Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Contribution -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h1 class="title is-3">Contributions</h1>
        <div class="content has-text-justified">
          <p>
          <ul>
            <li>We propose the wide-spectrum human preference dataset HPDv3 by integrating high-quality real-world images and state-of-the-art generative model outputs, including 1.08M text-image pairs and 1.17M annotated pairwise comparisons. This serves as a nuanced benchmark for evaluating generative models.</li>
            <li>We introduce HPSv3, a human preference model trained with HPDv3, which leverages the feature of VLMs and is trained using an uncertainty-aware ranking loss to discern subtle differences in training samples.</li>
            <li>We introduce CoHP, a novel reasoning approach to enhance image generation quality by iteratively refining outputs using HPSv3.</li>
          </ul>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Contribution -->

<!-- Model Overview -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h1 class="title is-3">Model Architecture & COHP Method</h1>
        <div class="content has-text-justified">
          <div style="display: flex; justify-content: space-around; align-items: center; gap: 20px;">
            <img src="static/images/model_overview.png" style="max-width:45%; height:auto;">
            <div style="border-left: 2px solid #ccc; height: 200px;"></div>
            <img src="static/images/cohpoverview.png" style="max-width:50%; height:auto;">
          </div>
          <p>
            <div style="display: flex; justify-content: space-between; align-items: flex-start; gap: 20px;">
              <div style="flex: 1;">
                <strong>Left:</strong> HPSv3 employs a VLM backbone to extract rich semantic representations from images and captions, then utilizes uncertainty-aware ranking to effectively learn human preferences from paired comparison data.
              </div>
              <div style="flex: 1;">
                <strong>Right:</strong> CoHP incorporates both model preferences and sample references, selected through HPSv3, to build a thinking-and-choosing image generation process.
              </div>
            </div>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Contribution -->


<!-- Dataset overview -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-4">Comparison with Related Datasets</h2>
        <div class="content has-text-justified">
            <img src="static/images/datasetoverview.png" style="max-width:90%; height:auto; display:block; margin: 0 auto;">
          <p>
HPDv3 stands as the only dataset that covers all image generation model types and both
high quality images(HQI) and low quality images(LQI) with diverse prompts and the largest image collection. Its annotations show high
convergence among evaluators, ensuring exceptional reliability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Dataset Overview -->


<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-4">Preference Prediction Accuracy</h2>
        <div class="content has-text-justified">
          <img src="static/images/pref_acc.png" style="max-width:45%; height:auto; display:block; margin: 0 auto;">
          <p>
          Preference prediction accuracy (%) on test sets of
ImageReward, HPDv2 and HPDv3 Benchmark, with best and
second-best results bolded and underlined. HPSv3 and HPDv3
exhibit exceptional confidence in human preference.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-4">Image Generation Benchmark</h2>
        <div class="content has-text-justified">
          <img src="static/images/benchmark.png" style="max-width:100%; height:auto; display:block; margin: 0 auto;">
          <p>
HPDv3 Benchmark of popular image generation models with HPDv3 benchmark in 12 categories. We generate images for
each model using its recommended inference settings or via its API. The best and the second best results are bolded and underlined.

          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-4">Benchmark Results Comparison</h2>
        <div class="content has-text-justified">
          <div style="display: flex; justify-content: space-around; align-items: center; gap: 20px;">
            <img src="static/images/benchmark_diff_models.png" style="max-width:48%; height:auto;">
            <img src="static/images/metric_to_humananno.png" style="max-width:48%; height:auto;">
          </div>
          <p>
 HPSv3 shows
the highest correlation with human annotation, indicating its supe-
rior performance in reflecting human preferences.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- COHP Visualization -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-4">Chain-of-Human-Preference (CoHP) Visualization</h2>
        <div class="content has-text-justified">
          <p>
            Chain-of-Human-Preference (CoHP) is an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step.
          </p>
          <img src="static/images/cohp/cohp1.jpg" style="max-width:100%; height:auto; display:block; margin: 20px auto;">
          <img src="static/images/cohp/cohp2.jpg" style="max-width:100%; height:auto; display:block; margin: 20px auto;">
          <img src="static/images/cohp/cohp3.jpg" style="max-width:100%; height:auto; display:block; margin: 20px auto;">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End COHP Visualization -->


<!-- COHP Visualization -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-4">HPSv3 as Reward Model</h2>
        <div class="content has-text-justified">
          <p>
            We utilize HPSv3 as a reward model in DanceGRPO, a reinforcement learning framework, to enhance the quality of generated images. Compared to HPSv2, HPSv3 demonstrates superior performance in improving image quality and prompt alignment, and less reward hacking.
          </p>
          <img src="static/images/rl.jpg" style="max-width:100%; height:auto; display:block; margin: 20px auto; transform:translateX(5%);">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End COHP Visualization -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{hpsv3,
  title={HPSv3: Towards Wide-Spectrum Human Preference Score},
  author={Ma, Yuhang and Shui, Yunhao and Wu, Xiaoshi and Sun, Keqiang and Li, Hongsheng},
  journal={ICCV},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
